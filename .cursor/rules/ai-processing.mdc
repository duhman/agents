---
description: OpenAI Assistants API for dynamic extraction and response generation with vector store retrieval
globs: apps/agent/**/*.ts, packages/prompts/**/*.ts
alwaysApply: false
---

# OpenAI Assistants API Processing

This rule documents the Assistants API architecture for email extraction and dynamic response generation with automatic vector store retrieval.

## Architecture Overview

The system uses **OpenAI Assistants API** for both extraction and response generation:

1. **Extraction Assistant**: Analyzes emails and extracts structured cancellation data
2. **Response Assistant**: Generates dynamic, personalized customer responses
3. **Automatic RAG**: Both assistants automatically search the vector store via `file_search` tool
4. **Streaming Responses**: Response generation uses streaming for real-time output
5. **Thread Management**: Conversations managed via threads and runs

## Core Components

### 1. Assistant Configuration (`assistant-config.ts`)

Define assistants with system instructions and vector store configuration:

```typescript
export function getExtractionAssistantConfig(): AssistantConfig {
  return {
    name: "Elaway Cancellation Extraction Assistant",
    model: "gpt-4o-2024-08-06",
    temperature: 0, // Deterministic
    tools: [{ type: "file_search" }],
    tool_resources: {
      file_search: {
        vector_store_ids: [process.env.OPENAI_VECTOR_STORE_ID!]
      }
    },
    instructions: `You are an expert email analyzer...
    
ALWAYS search the vector store for similar cases before extraction.
...`
  };
}

export function getResponseAssistantConfig(): AssistantConfig {
  return {
    name: "Elaway Customer Response Assistant",
    model: "gpt-4o-2024-08-06",
    temperature: 0.3, // Creative but controlled
    tools: [{ type: "file_search" }],
    tool_resources: {
      file_search: {
        vector_store_ids: [process.env.OPENAI_VECTOR_STORE_ID!]
      }
    },
    instructions: `You are Elaway's customer service assistant...
    
ALWAYS search the vector store for similar cases and policy information.
...`
  };
}
```

### 2. Assistants Processor (`assistants-processor.ts`)

Implement the core workflow:

```typescript
// Cache assistants to avoid recreation
let extractionAssistantId: string | null = null;
let responseAssistantId: string | null = null;

async function extractWithAssistant(maskedEmail: string, logContext: LogContext): Promise<ExtractionResultEnhanced> {
  // Get or create extraction assistant
  extractionAssistantId = await getOrCreateAssistant(
    getExtractionAssistantConfig(),
    extractionAssistantId,
    logContext
  );

  // Create thread with email
  const thread = await openai.beta.threads.create({
    messages: [{
      role: "user",
      content: `Analyze this customer email and extract structured information as JSON:\n\n${maskedEmail}`
    }]
  });

  // Run assistant (automatically uses file_search)
  const run = await openai.beta.threads.runs.createAndPoll(thread.id, {
    assistant_id: extractionAssistantId
  });

  if (run.status !== "completed") {
    throw new Error(`Extraction run failed with status: ${run.status}`);
  }

  // Get response and parse JSON
  const messages = await openai.beta.threads.messages.list(thread.id);
  const assistantMessage = messages.data[0];
  
  const textContent = assistantMessage.content.find(c => c.type === "text");
  const responseText = textContent.text.value;
  const jsonMatch = responseText.match(/\{[\s\S]*\}/);
  
  return extractionSchemaEnhanced.parse(JSON.parse(jsonMatch[0]));
}

async function generateResponseWithAssistant(extraction: ExtractionResultEnhanced, maskedEmail: string, logContext: LogContext): Promise<string> {
  // Get or create response assistant
  responseAssistantId = await getOrCreateAssistant(
    getResponseAssistantConfig(),
    responseAssistantId,
    logContext
  );

  // Create thread
  const thread = await openai.beta.threads.create({
    messages: [{
      role: "user",
      content: `Generate a professional customer response for this cancellation request...`
    }]
  });

  // Stream the response
  const stream = openai.beta.threads.runs.stream(thread.id, {
    assistant_id: responseAssistantId
  });

  let fullResponse = "";
  for await (const event of stream) {
    if (event.event === "thread.message.delta") {
      const delta = event.data.delta;
      if (delta.content?.[0]?.type === "text") {
        fullResponse += delta.content[0].text.value;
      }
    }
  }

  return fullResponse.trim();
}
```

## Best Practices

### Vector Store Integration

✅ **DO:** Let assistants automatically search via file_search tool

```typescript
// Assistants API handles vector store search automatically
const assistant = await openai.beta.assistants.create({
  tools: [{ type: "file_search" }],
  tool_resources: {
    file_search: {
      vector_store_ids: [vectorStoreId]
    }
  }
});

// Assistant will use file_search during run.createAndPoll()
```

❌ **DON'T:** Manually fetch and inject vector store context

Vector store retrieval is handled automatically by the Assistants API file_search tool.

### Thread Management

✅ **DO:** Create new threads for each request

```typescript
const thread = await openai.beta.threads.create({
  messages: [{ role: "user", content: userMessage }]
});

const run = await openai.beta.threads.runs.createAndPoll(thread.id, {
  assistant_id: assistantId
});
```

❌ **DON'T:** Reuse threads across different requests

Each request should have its own thread for isolation and proper context management.

### Response Streaming

✅ **DO:** Use streaming for real-time response generation

```typescript
const stream = openai.beta.threads.runs.stream(thread.id, {
  assistant_id: assistantId
});

for await (const event of stream) {
  if (event.event === "thread.message.delta") {
    // Process streamed content
  }
}
```

❌ **DON'T:** Use createAndPoll() for response generation

Use streaming for better real-time performance and user experience.

### Temperature Settings

- **Extraction**: `temperature: 0` (deterministic, consistent output)
- **Response Generation**: `temperature: 0.3` (controlled creativity, personalization)

### Error Handling

✅ **DO:** Check run status and handle failures

```typescript
const run = await openai.beta.threads.runs.createAndPoll(thread.id, {
  assistant_id: assistantId
});

if (run.status !== "completed") {
  throw new Error(`Run failed with status: ${run.status}`);
}
```

❌ **DON'T:** Assume runs always complete successfully

Check status and handle failure cases appropriately.

## Processing Flow

```
Customer Email
    ↓
[PII Masking]
    ↓
[Extract with Assistants API]
  - Create thread
  - Run extraction assistant
  - Auto-search vector store via file_search
  - Parse JSON extraction
    ↓
[Validation]
  - Is cancellation?
  - Clear intent?
  - Sufficient data?
    ↓
[Create Ticket in Database]
    ↓
[Generate Response with Assistants API]
  - Create thread
  - Run response assistant
  - Stream response output
  - Auto-search vector store for similar cases
    ↓
[Save Draft and Metrics]
    ↓
[Post to Slack for Review]
```

## Configuration

### Environment Variables

```bash
# Required
OPENAI_API_KEY=sk-...
OPENAI_VECTOR_STORE_ID=vs_...

# Optional: Pre-configured assistant IDs (auto-creates if not provided)
OPENAI_EXTRACTION_ASSISTANT_ID=asst_...
OPENAI_RESPONSE_ASSISTANT_ID=asst_...
```

### Assistant Templates

Extraction assistant instructions should include:
- Task description
- Required extraction fields
- Classification rules
- Quality checks
- Vector store search guidance

Response assistant instructions should include:
- Policy guidelines
- Tone and style requirements
- Response structure
- Language support
- Edge case handling
- Vector store search guidance

## Performance Considerations

- **Latency**: Assistants API + vector store search adds ~3-5 seconds per request
- **Cost**: Higher API usage (both extraction and generation use Assistants API)
- **Reliability**: Automatic retry logic with exponential backoff (3 retries, 1s base delay)
- **Scaling**: Thread cleanup is automatic; assistants are cached per request lifecycle

## Monitoring & Metrics

Track these metrics:

```typescript
metricsCollector.record({
  extraction_method: "assistants-api",
  is_cancellation: extraction.is_cancellation,
  edge_case: extraction.edge_case,
  confidence,
  processing_time_ms: duration,
  policy_compliant: true,
  language: extraction.language,
  rag_context_used: true, // Always true with Assistants API
  has_payment_issue: extraction.has_payment_issue
});
```

## Testing

Test both assistants independently:

```typescript
// Test extraction
const extractionResult = await extractWithAssistant(maskedEmail, logContext);
expect(extractionResult.is_cancellation).toBe(true);
expect(extractionResult.reason).toBe("moving");

// Test response generation
const response = await generateResponseWithAssistant(extraction, maskedEmail, logContext);
expect(response.length).toBeGreaterThan(50);
expect(response.toLowerCase()).toContain("elaway");
```

## Troubleshooting

### Run fails with status "requires_action"

The assistant encountered a tool call (file_search) that needs to be handled:

```typescript
if (run.status === "requires_action") {
  // Submit tool results and continue polling
  // (handled automatically by createAndPoll)
}
```

### No JSON in extraction response

Ensure the extraction assistant instructions explicitly request JSON output and include proper formatting examples.

### Vector store searches returning no results

Check that:
1. Vector store ID is correct
2. Files are properly uploaded and indexed
3. Query is specific enough for semantic matching
4. File chunks contain relevant content

## References

- `apps/agent/src/assistant-config.ts` - Assistant configurations
- `apps/agent/src/assistants-processor.ts` - Main processor implementation
- `apps/agent/src/tests/assistants-processor.test.ts` - Comprehensive tests
- OpenAI Assistants API: https://platform.openai.com/docs/assistants
- Vector Stores: https://platform.openai.com/docs/assistants/tools/file-search
